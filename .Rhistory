x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
return(x)
}
#normalise data aggregates, pivots and normalises the relative frequency table
normalise_data <- function(x) {
x <- aggregate(. ~ year + word, x, sum)
x <- x %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
x[,2:dim(x)[2]] <- scale(x[,2:dim(x)[2]] / rowSums(x[,2:dim(x)[2]]))
return(x)
}
#return_dist_object gives us a relatively neatly separated out distance matrix
return_dist_object <- function(x) {
x <- dist.cosine(as.matrix(x))
x <- melt(as.matrix(x), variable.names(c("to", "from")))
colnames(x)[1:2] <- c("from", "to")
return(x)
}
#correlation analysis gives us a neat object that can be used to carry out a rudimentary topic model, i.e. filtering for words like 'love', or 'home' to see what terms correlate positively or negatively across time
correlation_analysis <- function(x) {
y <- x[, !sapply(x, is.character)]
y$year...1 <- NULL
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
return(y)
}
#load in our data
fiction_data <- read.csv("fiction_yearly_summary.csv", stringsAsFactors = F)
poetry_data <- read.csv("poetry_yearly_summary.csv", stringsAsFactors = F)
drama_data <- read.csv("drama_yearly_summary.csv", stringsAsFactors = F)
fiction_data <- fiction_data %>% filter(year >= 1724)
poetry_data <- poetry_data %>% filter(year >= 1746)
drama_data <- drama_data %>% filter(year >= 1749)
#read in roman numerals text file
roman_numerals <- read.csv("romannumerals.txt", stringsAsFactors = F)
#drop these columns as we do not need them
fiction_data$correctionapplied <- NULL
poetry_data$correctionapplied <- NULL
drama_data$correctionapplied <- NULL
#strip spaces and punctuation from the word columns
fiction_data$word <- gsub('[[:punct:]]+','', fiction_data$word)
poetry_data$word <- gsub('[[:punct:]]+','', poetry_data$word)
drama_data$word <- gsub('[[:punct:]]+','', drama_data$word)
fiction_data$word <- gsub("[[:space:]]", "", fiction_data$word)
poetry_data$word <- gsub('[[:space:]]','', poetry_data$word)
drama_data$word <- gsub('[[:space:]]','', drama_data$word)
#create alpha_vector which contains every letter of the alphabet, except a and i, an empty element, roman numerals and a few other artefacts we want to tidy up
alpha_vector <- c("", "ALLTOKENS", "DICTIONARYWORD", "ALPHABETIC", letters[-c(1, 9)], roman_numerals[,1], "NULL", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9")
#remove every element in alpha vector from the data
fiction_data <- fiction_data %>% filter(!word %in% alpha_vector)
poetry_data <- poetry_data %>% filter(!word %in% alpha_vector)
drama_data <- drama_data %>% filter(!word %in% alpha_vector)
fiction_data <- fiction_data %>% filter(!word %in% 1:1000)
poetry_data <- poetry_data %>% filter(!word %in% 1:1000)
drama_data <- drama_data %>% filter(!word %in% 1:1000)
fiction_data <- fiction_data %>% group_by(year, word) %>% summarise(termfreq = sum(termfreq))
totals <- fiction_data %>% group_by(year) %>% summarise(total = sum(termfreq))
fiction_data <- full_join(fiction_data, totals, by = "year")
fiction_data$termfreq <- (fiction_data$termfreq / fiction_data$total) * 100
fiction_data$total <- NULL
fiction_data <- fiction_data %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
fiction_data[,2:dim(fiction_data)[2]] <- scale(fiction_data[,2:dim(fiction_data)[2]])
y <- fiction_data[, !sapply(fiction_data, is.character)]
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(row == "year...1" & y >= 0.7 | y <= -0.7 & col != "year...1")
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
y
#load the libraries we need
library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(stylo)
library(glmnet)
#clean_data highlights elements in the word vector we want rid of
clean_data <- function(x) {
x <- gsub('[[:punct:]]+','', x)
x <- gsub("[[:space:]]", "", x)
x[which(grepl("0", x))] <- "NULL"
x[which(grepl("1", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
return(x)
}
#normalise data aggregates, pivots and normalises the relative frequency table
normalise_data <- function(x) {
x <- aggregate(. ~ year + word, x, sum)
x <- x %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
x[,2:dim(x)[2]] <- scale(x[,2:dim(x)[2]] / rowSums(x[,2:dim(x)[2]]))
return(x)
}
#return_dist_object gives us a relatively neatly separated out distance matrix
return_dist_object <- function(x) {
x <- dist.cosine(as.matrix(x))
x <- melt(as.matrix(x), variable.names(c("to", "from")))
colnames(x)[1:2] <- c("from", "to")
return(x)
}
#correlation analysis gives us a neat object that can be used to carry out a rudimentary topic model, i.e. filtering for words like 'love', or 'home' to see what terms correlate positively or negatively across time
correlation_analysis <- function(x) {
y <- x[, !sapply(x, is.character)]
y$year...1 <- NULL
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
return(y)
}
#load in our data
fiction_data <- read.csv("fiction_yearly_summary.csv", stringsAsFactors = F)
poetry_data <- read.csv("poetry_yearly_summary.csv", stringsAsFactors = F)
drama_data <- read.csv("drama_yearly_summary.csv", stringsAsFactors = F)
fiction_data <- fiction_data %>% filter(year >= 1724)
poetry_data <- poetry_data %>% filter(year >= 1746)
drama_data <- drama_data %>% filter(year >= 1749)
#read in roman numerals text file
roman_numerals <- read.csv("romannumerals.txt", stringsAsFactors = F)
#drop these columns as we do not need them
fiction_data$correctionapplied <- NULL
poetry_data$correctionapplied <- NULL
drama_data$correctionapplied <- NULL
#strip spaces and punctuation from the word columns
fiction_data$word <- gsub('[[:punct:]]+','', fiction_data$word)
poetry_data$word <- gsub('[[:punct:]]+','', poetry_data$word)
drama_data$word <- gsub('[[:punct:]]+','', drama_data$word)
fiction_data$word <- gsub("[[:space:]]", "", fiction_data$word)
poetry_data$word <- gsub('[[:space:]]','', poetry_data$word)
drama_data$word <- gsub('[[:space:]]','', drama_data$word)
#create alpha_vector which contains every letter of the alphabet, except a and i, an empty element, roman numerals and a few other artefacts we want to tidy up
alpha_vector <- c("", "ALLTOKENS", "DICTIONARYWORD", "ALPHABETIC", letters[-c(1, 9)], roman_numerals[,1], "NULL", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9")
#remove every element in alpha vector from the data
fiction_data <- fiction_data %>% filter(!word %in% alpha_vector)
poetry_data <- poetry_data %>% filter(!word %in% alpha_vector)
drama_data <- drama_data %>% filter(!word %in% alpha_vector)
fiction_data <- fiction_data %>% filter(!word %in% 1:1000)
poetry_data <- poetry_data %>% filter(!word %in% 1:1000)
drama_data <- drama_data %>% filter(!word %in% 1:1000)
fiction_data <- fiction_data %>% group_by(year, word) %>% summarise(termfreq = sum(termfreq))
totals <- fiction_data %>% group_by(year) %>% summarise(total = sum(termfreq))
fiction_data <- full_join(fiction_data, totals, by = "year")
fiction_data$termfreq <- (fiction_data$termfreq / fiction_data$total) * 100
fiction_data$total <- NULL
fiction_data <- fiction_data %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
fiction_data[,2:dim(fiction_data)[2]] <- scale(fiction_data[,2:dim(fiction_data)[2]])
y <- fiction_data[, !sapply(fiction_data, is.character)]
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(row == "year...1" & y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
y
#load the libraries we need
library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(stylo)
library(glmnet)
#clean_data highlights elements in the word vector we want rid of
clean_data <- function(x) {
x <- gsub('[[:punct:]]+','', x)
x <- gsub("[[:space:]]", "", x)
x[which(grepl("0", x))] <- "NULL"
x[which(grepl("1", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
return(x)
}
#normalise data aggregates, pivots and normalises the relative frequency table
normalise_data <- function(x) {
x <- aggregate(. ~ year + word, x, sum)
x <- x %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
x[,2:dim(x)[2]] <- scale(x[,2:dim(x)[2]] / rowSums(x[,2:dim(x)[2]]))
return(x)
}
#return_dist_object gives us a relatively neatly separated out distance matrix
return_dist_object <- function(x) {
x <- dist.cosine(as.matrix(x))
x <- melt(as.matrix(x), variable.names(c("to", "from")))
colnames(x)[1:2] <- c("from", "to")
return(x)
}
#correlation analysis gives us a neat object that can be used to carry out a rudimentary topic model, i.e. filtering for words like 'love', or 'home' to see what terms correlate positively or negatively across time
correlation_analysis <- function(x) {
y <- x[, !sapply(x, is.character)]
y$year...1 <- NULL
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
return(y)
}
#load in our data
fiction_data <- read.csv("fiction_yearly_summary.csv", stringsAsFactors = F)
poetry_data <- read.csv("poetry_yearly_summary.csv", stringsAsFactors = F)
drama_data <- read.csv("drama_yearly_summary.csv", stringsAsFactors = F)
fiction_data <- fiction_data %>% filter(year >= 1724)
poetry_data <- poetry_data %>% filter(year >= 1746)
drama_data <- drama_data %>% filter(year >= 1749)
#read in roman numerals text file
roman_numerals <- read.csv("romannumerals.txt", stringsAsFactors = F)
#drop these columns as we do not need them
fiction_data$correctionapplied <- NULL
poetry_data$correctionapplied <- NULL
drama_data$correctionapplied <- NULL
#strip spaces and punctuation from the word columns
fiction_data$word <- gsub('[[:punct:]]+','', fiction_data$word)
poetry_data$word <- gsub('[[:punct:]]+','', poetry_data$word)
drama_data$word <- gsub('[[:punct:]]+','', drama_data$word)
fiction_data$word <- gsub("[[:space:]]", "", fiction_data$word)
poetry_data$word <- gsub('[[:space:]]','', poetry_data$word)
drama_data$word <- gsub('[[:space:]]','', drama_data$word)
#create alpha_vector which contains every letter of the alphabet, except a and i, an empty element, roman numerals and a few other artefacts we want to tidy up
alpha_vector <- c("", "ALLTOKENS", "DICTIONARYWORD", "ALPHABETIC", letters[-c(1, 9)], roman_numerals[,1], "NULL", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9")
#remove every element in alpha vector from the data
fiction_data <- fiction_data %>% filter(!word %in% alpha_vector)
poetry_data <- poetry_data %>% filter(!word %in% alpha_vector)
drama_data <- drama_data %>% filter(!word %in% alpha_vector)
fiction_data <- fiction_data %>% filter(!word %in% 1:1000)
poetry_data <- poetry_data %>% filter(!word %in% 1:1000)
drama_data <- drama_data %>% filter(!word %in% 1:1000)
fiction_data <- fiction_data %>% group_by(year, word) %>% summarise(termfreq = sum(termfreq))
totals <- fiction_data %>% group_by(year) %>% summarise(total = sum(termfreq))
fiction_data <- full_join(fiction_data, totals, by = "year")
fiction_data$termfreq <- (fiction_data$termfreq / fiction_data$total) * 100
fiction_data$total <- NULL
fiction_data <- fiction_data %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
fiction_data[,2:dim(fiction_data)[2]] <- scale(fiction_data[,2:dim(fiction_data)[2]])
y <- fiction_data[, !sapply(fiction_data, is.character)]
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y <- y %>% filter(row == "year...1")
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
y
pos_corr <- y %>% filter(y > 0)
pos_corr
neg_corr <- y %>% filter(y < 0)
neg_corr
neg_corr$col
pos_corr$col
pos_corr$col[1000]
pos_corr$col[1001:2000]
install.packages("tidytet")
install.packages("tidytext")
library(tidytext)
textstem::lemmatisewords
textstem::lemmatize_words
install.packages("textstem")
library(textstem)
fiction_data
?lemmatize_words
#load the libraries we need
library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(stylo)
library(glmnet)
#clean_data highlights elements in the word vector we want rid of
clean_data <- function(x) {
x <- gsub('[[:punct:]]+','', x)
x <- gsub("[[:space:]]", "", x)
x[which(grepl("0", x))] <- "NULL"
x[which(grepl("1", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
return(x)
}
#normalise data aggregates, pivots and normalises the relative frequency table
normalise_data <- function(x) {
x <- aggregate(. ~ year + word, x, sum)
x <- x %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
x[,2:dim(x)[2]] <- scale(x[,2:dim(x)[2]] / rowSums(x[,2:dim(x)[2]]))
return(x)
}
#return_dist_object gives us a relatively neatly separated out distance matrix
return_dist_object <- function(x) {
x <- dist.cosine(as.matrix(x))
x <- melt(as.matrix(x), variable.names(c("to", "from")))
colnames(x)[1:2] <- c("from", "to")
return(x)
}
#correlation analysis gives us a neat object that can be used to carry out a rudimentary topic model, i.e. filtering for words like 'love', or 'home' to see what terms correlate positively or negatively across time
correlation_analysis <- function(x) {
y <- x[, !sapply(x, is.character)]
y$year...1 <- NULL
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
return(y)
}
#load in our data
fiction_data <- read.csv("fiction_yearly_summary.csv", stringsAsFactors = F)
poetry_data <- read.csv("poetry_yearly_summary.csv", stringsAsFactors = F)
drama_data <- read.csv("drama_yearly_summary.csv", stringsAsFactors = F)
fiction_data <- fiction_data %>% filter(year >= 1724)
poetry_data <- poetry_data %>% filter(year >= 1746)
drama_data <- drama_data %>% filter(year >= 1749)
#read in roman numerals text file
roman_numerals <- read.csv("romannumerals.txt", stringsAsFactors = F)
#drop these columns as we do not need them
fiction_data$correctionapplied <- NULL
poetry_data$correctionapplied <- NULL
drama_data$correctionapplied <- NULL
#strip spaces and punctuation from the word columns
fiction_data$word <- gsub('[[:punct:]]+','', fiction_data$word)
poetry_data$word <- gsub('[[:punct:]]+','', poetry_data$word)
drama_data$word <- gsub('[[:punct:]]+','', drama_data$word)
fiction_data$word <- gsub("[[:space:]]", "", fiction_data$word)
poetry_data$word <- gsub('[[:space:]]','', poetry_data$word)
drama_data$word <- gsub('[[:space:]]','', drama_data$word)
#create alpha_vector which contains every letter of the alphabet, except a and i, an empty element, roman numerals and a few other artefacts we want to tidy up
alpha_vector <- c("", "ALLTOKENS", "DICTIONARYWORD", "ALPHABETIC", letters[-c(1, 9)], roman_numerals[,1], "NULL", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9")
#remove every element in alpha vector from the data
fiction_data <- fiction_data %>% filter(!word %in% alpha_vector)
poetry_data <- poetry_data %>% filter(!word %in% alpha_vector)
drama_data <- drama_data %>% filter(!word %in% alpha_vector)
fiction_data <- fiction_data %>% filter(!word %in% 1:1000)
poetry_data <- poetry_data %>% filter(!word %in% 1:1000)
drama_data <- drama_data %>% filter(!word %in% 1:1000)
lemmatize_words(fiction_data$word)
length(unique(lemmatize_words(fiction_data$word)))
length(unique(fiction_data$word))
#load the libraries we need
library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)
library(stylo)
library(glmnet)
#clean_data highlights elements in the word vector we want rid of
clean_data <- function(x) {
x <- gsub('[[:punct:]]+','', x)
x <- gsub("[[:space:]]", "", x)
x[which(grepl("0", x))] <- "NULL"
x[which(grepl("1", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("?", x))] <- "NULL"
x[which(grepl("2", x))] <- "NULL"
x[which(grepl("3", x))] <- "NULL"
x[which(grepl("4", x))] <- "NULL"
x[which(grepl("5", x))] <- "NULL"
x[which(grepl("6", x))] <- "NULL"
x[which(grepl("7", x))] <- "NULL"
x[which(grepl("8", x))] <- "NULL"
x[which(grepl("9", x))] <- "NULL"
return(x)
}
#normalise data aggregates, pivots and normalises the relative frequency table
normalise_data <- function(x) {
x <- aggregate(. ~ year + word, x, sum)
x <- x %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
x[,2:dim(x)[2]] <- scale(x[,2:dim(x)[2]] / rowSums(x[,2:dim(x)[2]]))
return(x)
}
#return_dist_object gives us a relatively neatly separated out distance matrix
return_dist_object <- function(x) {
x <- dist.cosine(as.matrix(x))
x <- melt(as.matrix(x), variable.names(c("to", "from")))
colnames(x)[1:2] <- c("from", "to")
return(x)
}
#correlation analysis gives us a neat object that can be used to carry out a rudimentary topic model, i.e. filtering for words like 'love', or 'home' to see what terms correlate positively or negatively across time
correlation_analysis <- function(x) {
y <- x[, !sapply(x, is.character)]
y$year...1 <- NULL
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
return(y)
}
#load in our data
fiction_data <- read.csv("fiction_yearly_summary.csv", stringsAsFactors = F)
poetry_data <- read.csv("poetry_yearly_summary.csv", stringsAsFactors = F)
drama_data <- read.csv("drama_yearly_summary.csv", stringsAsFactors = F)
fiction_data <- fiction_data %>% filter(year >= 1724)
poetry_data <- poetry_data %>% filter(year >= 1746)
drama_data <- drama_data %>% filter(year >= 1749)
#read in roman numerals text file
roman_numerals <- read.csv("romannumerals.txt", stringsAsFactors = F)
#drop these columns as we do not need them
fiction_data$correctionapplied <- NULL
poetry_data$correctionapplied <- NULL
drama_data$correctionapplied <- NULL
#strip spaces and punctuation from the word columns
fiction_data$word <- gsub('[[:punct:]]+','', fiction_data$word)
poetry_data$word <- gsub('[[:punct:]]+','', poetry_data$word)
drama_data$word <- gsub('[[:punct:]]+','', drama_data$word)
fiction_data$word <- gsub("[[:space:]]", "", fiction_data$word)
poetry_data$word <- gsub('[[:space:]]','', poetry_data$word)
drama_data$word <- gsub('[[:space:]]','', drama_data$word)
#create alpha_vector which contains every letter of the alphabet, except a and i, an empty element, roman numerals and a few other artefacts we want to tidy up
alpha_vector <- c("", "ALLTOKENS", "DICTIONARYWORD", "ALPHABETIC", letters[-c(1, 9)], roman_numerals[,1], "NULL", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9")
#remove every element in alpha vector from the data
fiction_data <- fiction_data %>% filter(!word %in% alpha_vector)
poetry_data <- poetry_data %>% filter(!word %in% alpha_vector)
drama_data <- drama_data %>% filter(!word %in% alpha_vector)
fiction_data <- fiction_data %>% filter(!word %in% 1:1000)
poetry_data <- poetry_data %>% filter(!word %in% 1:1000)
drama_data <- drama_data %>% filter(!word %in% 1:1000)
fiction_data$word <- lemmatize_words(fiction_data$word)
fiction_data <- fiction_data %>% group_by(year, word) %>% summarise(termfreq = sum(termfreq))
totals <- fiction_data %>% group_by(year) %>% summarise(total = sum(termfreq))
fiction_data <- full_join(fiction_data, totals, by = "year")
fiction_data$termfreq <- (fiction_data$termfreq / fiction_data$total) * 100
fiction_data$total <- NULL
fiction_data <- fiction_data %>% pivot_wider(names_from = word, values_from = termfreq, values_fill = 0, names_repair = "unique")
fiction_data[,2:dim(fiction_data)[2]] <- scale(fiction_data[,2:dim(fiction_data)[2]])
y <- fiction_data[, !sapply(fiction_data, is.character)]
y <- as.data.frame(y)
y[,apply(y, 2, sd) == 0] <- NULL
y <- cor(y)
y <- na.omit(as_tibble(data.frame(row=rownames(y)[row(y)], col=colnames(y)[col(y)], y=c(y))))
y <- y %>% filter(y >= 0.7 | y <= -0.7)
y <- y %>% filter(row == "year...1")
y$y <- round(y$y, 2)
y <- y[!y$row == y$col,]
y
pos_words <- y %>% filter(y > 0)
pos_words$col
neg_words <- y %>% filter(y < 0)
neg_words
neg_words$col
